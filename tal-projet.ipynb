{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# Cours TAL – Mini-projet\n",
    "# Classification de dépêches d’agence avec NLTK\n",
    "\n",
    "> Tiago Povoa Quinteiro\n",
    "\n",
    "**Modalités du projet**\n",
    "\n",
    "L’objectif de ce projet est de réaliser des expériences de classification de documents sous NLTK avec\n",
    "le corpus de dépêches Reuters. Le projet est individuel : vous êtes responsable des différentes options\n",
    "choisies, et en principe les résultats de chaque projet seront différents. Le projet sera jugé sur la\n",
    "qualité des expériences (correction méthodologique) mais aussi sur la discussion des options\n",
    "explorées.\n",
    "\n",
    "Vous devez remettre un notebook Jupyter présentant vos choix, votre code, vos résultats et les\n",
    "discussions. Le notebook devra déjà contenir les résultats des exécutions, mais pourra être ré-exécuté\n",
    "par le professeur en vue d’une vérification.\n",
    "\n",
    "Vous devrez en outre faire une courte présentation orale (5-7 min.) et répondre aux questions sur\n",
    "votre projet (5-7 min.) lors d’une séance sur Teams (15 min.) avec le professeur et l’assistant.\n",
    "\n",
    "**Description des expériences**\n",
    "\n",
    "1. **L’objectif général** est d’explorer au moins deux aspects parmi les multiples choix qui se posent lors de la création d’un système de classification de textes.\n",
    "2. **Données** : les dépêches du corpus Reuters, tel qu’il est fourni par NLTK. Vous respecterez notamment la division en données d’entraînement (train) et données de test.\n",
    "3. **Hyper-paramètres** : la définition d’un classifieur comporte un grand nombre de choix de conception, dans plusieurs dimensions. Dans ce projet, et pour chaque objectif de classification (voir ci-dessous) vous explorerez deux dimensions. Pour chaque dimension, vous comparerez au moins deux options pour trouver laquelle fournit le meilleur score, et vous tenterez d’expliquer pourquoi. Vous pourrez choisir parmi les options suivantes :\n",
    "\n",
    "    a. options de prétraitement des textes : stopwords, lemmatisation, tout en minuscules.\n",
    "    \n",
    "    b. options de représentation : présence/absence de mots indicateurs, nombre de mots indicateurs ; présence/absence/nombre de bigrammes, trigrammes ; autres traits : longueur de la dépêche, rapport tokens/types.\n",
    "    \n",
    "    c. classifieurs et leurs paramètres : divers choix possibles (voir la documentation).\n",
    "    \n",
    "    \n",
    "4. **Objectif de classification** : vous devrez construire quatre classifieurs. Vous choisirez les meilleurs hyper-paramètres pour chaque classifieur sans regarder les résultats sur les données de test NLTK, mais en divisant les données d’entraînement NLTK en 80% train et 20% dev. Vous ferez ensuite l’entraînement final sur l’intégralité des données d’entraînement.\n",
    "\n",
    "    a. Veuillez d’abord définir et entraîner trois classifieurs binaires, correspondant à trois catégories de votre choix. Chaque classifieur prédit si une dépêche appartient ou non à la catégorie, i.e. si elle doit recevoir ou non l’étiquette respective. Veuillez construire un premier classifieur binaire pour une étiquette que vous choisirez librement parmi les trois suivantes : ‘money-fx’, ‘interest’, ou ‘money-supply’. Le deuxième classifieur binaire concernera une étiquette de votre choix parmi : ‘grain’, ‘wheat’, ‘corn’. Enfin, le troisième sera choisi parmi : ‘crude’, ‘nat-gas’, ‘gold’.\n",
    "        - Veuillez donner les scores de rappel, précision et f-mesure de chacun des trois classifieurs que vous avez conçus et entraînés.\n",
    "    \n",
    "    b. On vous demande également de définir un quatrième classifieur qui assigne l’une des trois étiquettes que vous avez choisies ci-dessus plus la catégorie ‘other’ (il assigne donc une seule étiquette parmi quatre). Vous devrez adapter légèrement les données, car un très petit nombre de dépêches (combien ?) sont en réalité annotées avec plusieurs de ces étiquettes, et vous n’en retiendrez que la première.\n",
    "        - Veuillez évaluer ce classifieur en termes de rappel, précision et f-mesure pour chacune des trois étiquettes choisies ci-dessus, et comparer ces trois scores à ceux des trois classifieurs binaires précédents.\n",
    "    \n",
    "5. **Documentation** : livre NLTK, chapitre 2 pour le corpus Reuters, chapitre 6 pour la classification, et http://www.nltk.org/howto/classify.html pour les classifieurs dans NLTK ; Introduction to Information Retrieval (https://nlp.stanford.edu/IR-book/information-retrieval-book.html), chapitre 13, pour une discussion de méthodes de classification, et des exemples de scores obtenus sur certaines étiquettes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Here are some util functions to remove stop words, lemmatization, punctuation, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    \"\"\"\n",
    "    Remove english stopwords and words of length 1\n",
    "    inspiration: http://www.nltk.org/book/ch02.html\n",
    "    words: a list of words\n",
    "    return words lower case without stopwords\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [w.lower() for w in words if len(w) > 1 and w.lower() not in stopwords]\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"\n",
    "    inspiration: https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "    \"\"\"\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return [w.translate(table) for w in words]\n",
    "\n",
    "\n",
    "def do_lemmatize(words):\n",
    "    wnl = WordNetLemmatizer() \n",
    "    return [wnl.lemmatize(w) for w in words]\n",
    "\n",
    "\n",
    "def filter_words(words, rm_punctuation, rm_stopwords, lemmatize):\n",
    "    if rm_punctuation:\n",
    "        words = remove_punctuation(words)\n",
    "        \n",
    "    if lemmatize:\n",
    "        words = do_lemmatize(words)\n",
    "\n",
    "    if rm_stopwords:\n",
    "        words = remove_stop_words(words)\n",
    "            \n",
    "    return words\n",
    "\n",
    "\n",
    "def get_reuters_vocab(sample_size, rm_punctuation, rm_stopwords, lemmatize):\n",
    "    \"\"\"\n",
    "    return a sample of words from reuters\n",
    "    \"\"\"\n",
    "    words = reuters.words()         \n",
    "    \n",
    "    fdist = FreqDist(words)\n",
    "    \n",
    "    print('Number of words in reuters: {} total - unique: {} \\n'.format(fdist.N(), fdist.B()))\n",
    "    \n",
    "    fdist = FreqDist(filter_words(words, rm_punctuation, rm_stopwords, lemmatize) )\n",
    "\n",
    "    print('After removals: {} total - unique: {} \\n'.format(fdist.N(), fdist.B()))\n",
    "    \n",
    "    return [w[0] for w in fdist.most_common(sample_size)]\n",
    "\n",
    "\n",
    "def vocab_dic(vocab):\n",
    "    \"\"\"\n",
    "    return a vocabulary list as a dictionnary with all values\n",
    "    set to False\n",
    "    \"\"\"\n",
    "    return {w:False for w in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _category(fileid, wanted_category):\n",
    "    \"\"\"\n",
    "    return a specific category given a fileid. If not found, returns \"not_category\"\n",
    "    \"\"\"\n",
    "    return wanted_category if wanted_category in reuters.categories(fileid) else f'not_{wanted_category}'\n",
    "\n",
    "\n",
    "def _words_dic(vocab_dic, words):\n",
    "    _tmp_dic = dict()\n",
    "    _tmp_dic.update(vocab_dic)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in vocab_dic:\n",
    "            _tmp_dic[w] = True\n",
    "            \n",
    "    return _tmp_dic\n",
    "            \n",
    "\n",
    "def prepare_reuters_data(vocab_dic, wanted_category, rm_punctuation, rm_stopwords, lemmatize):\n",
    "    __train_data = []\n",
    "    __test_data = []\n",
    "\n",
    "    for fileid in reuters.fileids():\n",
    "        category = _category(fileid, wanted_category)\n",
    "        words = reuters.words(fileid)\n",
    "        words = filter_words(words, rm_punctuation, rm_stopwords, lemmatize)\n",
    "        dic = _words_dic(vocab_dic, words)\n",
    "        if fileid.startswith('test'):\n",
    "            __train_data.append([dic, category])\n",
    "        else:\n",
    "            __test_data.append([dic, category])\n",
    "                \n",
    "    return __test_data, __test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769 7769\n",
      "6215 1554\n"
     ]
    }
   ],
   "source": [
    "def prepare_pre_cut(train_data, test_data):\n",
    "    \"\"\"\n",
    "    As asked by the assignement, we have to cut on train data\n",
    "    between 80% for training and 20% as dev (here named test)\n",
    "    \"\"\"\n",
    "    cut_train = int(len(train_data) * 0.8 )\n",
    "    # cut_test = int(len(train_data) * 0.2 )\n",
    "    return train_data[0:cut_train], train_data[cut_train:]\n",
    "\n",
    "\n",
    "reuters_train_data, reuters_test_data = prepare_reuters_data(reuters_vocabulary_dic, 'money-supply', True, True, True)\n",
    "print(len(reuters_train_data), len(reuters_test_data))\n",
    "\n",
    "cut_reuters_train_data, cut_reuters_test_data = prepare_pre_cut(reuters_train_data, reuters_test_data)\n",
    "print(len(cut_reuters_train_data), len(cut_reuters_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_2 = nltk.NaiveBayesClassifier.train(cut_reuters_train_data)\n",
    "# print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier_2, cut_reuters_test_data))*100)\n",
    "# classifier_2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "from nltk.metrics import precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data(wanted_category, cut, sampleSize, rm_punctuation, rm_stopwords, lemmatize):\n",
    "    _vocab_dic = vocab_dic(get_reuters_vocab(sampleSize, rm_punctuation, rm_stopwords,  lemmatize))\n",
    "    \n",
    "    _train_data, _test_data = prepare_reuters_data(_vocab_dic, wanted_category, rm_punctuation, rm_stopwords,  lemmatize)\n",
    "\n",
    "    if cut:\n",
    "        _train_data, _test_data = prepare_pre_cut(_train_data, _test_data)\n",
    "        \n",
    "    return _train_data, _test_data\n",
    "    \n",
    "    \n",
    "def eval_classifier(trained_classifier, test_data, label):\n",
    "    \"\"\"\n",
    "    Inspiration: https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/\n",
    "    \"\"\"\n",
    "    ref_set = collections.defaultdict(set)\n",
    "    test_set = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, true_label) in enumerate(test_data):\n",
    "        ref_set[true_label].add(i)\n",
    "        observed = trained_classifier.classify(feats)\n",
    "        \n",
    "        test_set[observed].add(i)\n",
    "\n",
    "    print('Classifier evaluation: ')\n",
    "    print( 'Precision:', precision(ref_set[label], test_set[label]) )\n",
    "    print( 'Recall:', recall(ref_set[label], test_set[label]) )\n",
    "    print( 'F-measure:', f_measure(ref_set[label], test_set[label]) )\n",
    "    \n",
    "    \n",
    "def run_classifier(classifier, wanted_category, cut=True, sampleSize=500, rm_punctuation=True, rm_stopwords=True, lemmatize=True):\n",
    "    train_data, test_data = obtain_data(wanted_category, cut, sampleSize, rm_punctuation, rm_stopwords,  lemmatize)\n",
    "    \n",
    "    print(f'{classifier}')\n",
    "    \n",
    "    trained_classifier = None\n",
    "    if f'{classifier}' == \"<class 'nltk.classify.maxent.MaxentClassifier'>\":\n",
    "        print('Hello Maxent')\n",
    "        trained_classifier = classifier.train(train_data, trace=3)\n",
    "    else:\n",
    "        trained_classifier = classifier.train(train_data)\n",
    "    \n",
    "    eval_classifier(trained_classifier, test_data, wanted_category)\n",
    "\n",
    "    \n",
    "categories = {\n",
    "    0: ['money-fx', 'interest', 'money-supply'],\n",
    "    1: ['grain', 'wheat', 'corn'],\n",
    "    2: ['crude', 'nat-gas', 'gold']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4a\n",
    "\n",
    "## Class 1 - Naive Bayes classifier\n",
    "\n",
    "> Chosen category: money-fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.naivebayes.NaiveBayesClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 0.2757009345794392\n",
      "Recall: 0.4306569343065693\n",
      "F-measure: 0.3361823361823361\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "# Hyper paramters\n",
    "CUT=True\n",
    "SAMPLE_SIZE=10\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(NaiveBayesClassifier, categories[0][0], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples below, the data is always CUT=True\n",
    "The category word chosen was 'money-fx' since it was the first. Why not?\n",
    "\n",
    "### Test 1: Filtering\n",
    "\n",
    "#### Variant 1: No filter\n",
    "\n",
    "* SAMPLE_SIZE=5000\n",
    "\n",
    "Every other parameter to false\n",
    "\n",
    "\n",
    "* Precision: 0.3879003558718861\n",
    "* Recall: 0.7956204379562044\n",
    "* F-measure: 0.5215311004784688\n",
    "\n",
    "#### Variant 2: Punctuation\n",
    "\n",
    "Adding the punctuation filter\n",
    "\n",
    "* Precision: 0.39636363636363636\n",
    "* Recall: 0.7956204379562044\n",
    "* F-measure: 0.529126213592233\n",
    "\n",
    "Adding the punctuation filter improves a little bit the precision.\n",
    "\n",
    "#### Variant 3: Stop word\n",
    "\n",
    "* Precision: 0.4703196347031963\n",
    "* Recall: 0.7518248175182481\n",
    "* F-measure: 0.5786516853932584\n",
    "\n",
    "It does significantly improve the precision.\n",
    "The recall is a bit lower, the F-Measure improves (since we improve one of it's parameters)\n",
    "\n",
    "#### Variant 4: Lemmatization\n",
    "\n",
    "* Precision: 0.39636363636363636\n",
    "* Recall: 0.7956204379562044\n",
    "* F-measure: 0.529126213592233\n",
    "\n",
    "This variant looks very similar to punctuation. We lose too much by not using stop words\n",
    "\n",
    "#### Variant 5: Lemmatization and Stop word\n",
    "\n",
    "* Precision: 0.46017699115044247\n",
    "* Recall: 0.7591240875912408\n",
    "* F-measure: 0.5730027548209367\n",
    "\n",
    "By adding lemmatization, we lose a tiny bit on precision and a tinier bit in recall.\n",
    "Since F-Measure is a metric using both, we shall use it to decide: It seems it does not improve.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The better F-Measure was variant 3 (punctuation, stop words, no lemmatization).\n",
    "\n",
    "### Test 2:  Vocabulary size\n",
    "\n",
    "#### 500 \n",
    "\n",
    "* Precision: 0.45564516129032256\n",
    "* Recall: 0.8248175182481752\n",
    "* F-measure: 0.587012987012987\n",
    "\n",
    "#### 1250\n",
    "\n",
    "* Precision: 0.4826086956521739\n",
    "* Recall: 0.8102189781021898\n",
    "* F-measure: 0.6049046321525885\n",
    "\n",
    "#### 2500\n",
    "\n",
    "* Precision: 0.5045871559633027\n",
    "* Recall: 0.8029197080291971\n",
    "* F-measure: 0.6197183098591549\n",
    "\n",
    "#### 5000\n",
    "\n",
    "* Precision: 0.4703196347031963\n",
    "* Recall: 0.7518248175182481\n",
    "* F-measure: 0.5786516853932584\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The best score was obtained with 2500 in Vocabulary size and by filtering stop words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 2 - Decision Tree Classifier\n",
    "\n",
    "> chosen category: corn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 1720901 total - unique: 39425 \n",
      "\n",
      "<class 'nltk.classify.decisiontree.DecisionTreeClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 0.84\n",
      "Recall: 0.6774193548387096\n",
      "F-measure: 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "# Hyper paramters\n",
    "CUT=True\n",
    "SAMPLE_SIZE=1250\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=False\n",
    "LEMMATIZE=True\n",
    "\n",
    "run_classifier(DecisionTreeClassifier, categories[1][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Vocabulary size\n",
    "\n",
    "#### Variant A: 5000 all filters\n",
    "\n",
    "* Precision: 0.9333333333333333\n",
    "* Recall: 0.9032258064516129\n",
    "* F-measure: 0.9180327868852458\n",
    "\n",
    "#### Variant B: 2500 all filters\n",
    "\n",
    "* Precision: 0.9655172413793104\n",
    "* Recall: 0.9032258064516129\n",
    "* F-measure: 0.9333333333333333\n",
    "\n",
    "#### Variant C: 1250 all filters\n",
    "\n",
    "* Precision: 0.9666666666666667\n",
    "* Recall: 0.9354838709677419\n",
    "* F-measure: 0.9508196721311474\n",
    "\n",
    "#### Variant D: 500 All filters\n",
    "\n",
    "* Precision: 1.0\n",
    "* Recall: 0.6774193548387096\n",
    "* F-measure: 0.8076923076923077\n",
    "\n",
    "### Test 2: \n",
    "\n",
    "* SAMPLE_SIZE=1250\n",
    "\n",
    "#### Variant A: No filters\n",
    "\n",
    "* Precision: 0.84\n",
    "* Recall: 0.6774193548387096\n",
    "* F-measure: 0.7499999999999999\n",
    "\n",
    "#### Variant B: Punctuation\n",
    "\n",
    "* Precision: 0.84\n",
    "* Recall: 0.6774193548387096\n",
    "* F-measure: 0.7499999999999999\n",
    "\n",
    "Removing punctuation didn't change anything.\n",
    "\n",
    "#### Variant C: Stop words\n",
    "\n",
    "* Precision: 0.9666666666666667\n",
    "* Recall: 0.9354838709677419\n",
    "* F-measure: 0.9508196721311474\n",
    "\n",
    "It is totally identical to the variant with all filters.\n",
    "\n",
    "#### Variant D: Lemmatization (no stop words filtering)\n",
    "\n",
    "* Precision: 0.84\n",
    "* Recall: 0.6774193548387096\n",
    "* F-measure: 0.7499999999999999\n",
    "\n",
    "Adding lemmatization didn't help.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The only parameter who does help it the decision tree is removing stop words.\n",
    "Adding the two other filters doesn't seem to change the scores.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Best parameters: 1250 vocabulary size, Stop words on (you can add the other filters but it doesn't seem to change something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 3 - MaxentClassifier\n",
    "> chose category: gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 961132 total - unique: 29649 \n",
      "\n",
      "<class 'nltk.classify.maxent.MaxentClassifier'>\n",
      "Hello Maxent\n",
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.988\n",
      "             2          -0.11210        0.988\n",
      "             3          -0.03275        0.988\n",
      "             4          -0.01793        0.988\n",
      "             5          -0.01410        0.988\n",
      "             6          -0.01287        0.988\n",
      "             7          -0.01240        0.988\n",
      "             8          -0.01221        0.988\n",
      "             9          -0.01212        0.988\n",
      "            10          -0.01208        0.988\n",
      "            11          -0.01205        0.988\n",
      "            12          -0.01204        0.988\n",
      "            13          -0.01204        0.988\n",
      "            14          -0.01203        0.988\n",
      "            15          -0.01203        0.988\n",
      "            16          -0.01203        0.988\n",
      "            17          -0.01203        0.988\n",
      "            18          -0.01203        0.988\n",
      "            19          -0.01203        0.988\n",
      "            20          -0.01203        0.988\n",
      "            21          -0.01203        0.988\n",
      "            22          -0.01203        0.988\n",
      "            23          -0.01203        0.988\n",
      "            24          -0.01203        0.988\n",
      "            25          -0.01203        0.988\n",
      "            26          -0.01203        0.988\n",
      "            27          -0.01203        0.988\n",
      "            28          -0.01203        0.988\n",
      "            29          -0.01203        0.988\n",
      "            30          -0.01203        0.988\n",
      "            31          -0.01203        0.988\n",
      "            32          -0.01203        0.988\n",
      "            33          -0.01203        0.988\n",
      "            34          -0.01203        0.988\n",
      "            35          -0.01203        0.988\n",
      "            36          -0.01203        0.988\n",
      "            37          -0.01203        0.988\n",
      "            38          -0.01203        0.988\n",
      "            39          -0.01203        0.988\n",
      "            40          -0.01203        0.988\n",
      "            41          -0.01203        0.988\n",
      "            42          -0.01203        0.988\n",
      "            43          -0.01203        0.988\n",
      "            44          -0.01203        0.988\n",
      "            45          -0.01203        0.988\n",
      "            46          -0.01203        0.988\n",
      "            47          -0.01203        0.988\n",
      "            48          -0.01203        0.988\n",
      "            49          -0.01203        0.988\n",
      "            50          -0.01203        0.988\n",
      "            51          -0.01203        0.988\n",
      "            52          -0.01203        0.988\n",
      "            53          -0.01203        0.988\n",
      "            54          -0.01203        0.988\n",
      "            55          -0.01203        0.988\n",
      "            56          -0.01203        0.988\n",
      "            57          -0.01203        0.988\n",
      "            58          -0.01203        0.988\n",
      "            59          -0.01203        0.988\n",
      "            60          -0.01203        0.988\n",
      "            61          -0.01203        0.988\n",
      "            62          -0.01203        0.988\n",
      "            63          -0.01203        0.988\n",
      "            64          -0.01203        0.988\n",
      "            65          -0.01203        0.988\n",
      "            66          -0.01203        0.988\n",
      "            67          -0.01203        0.988\n",
      "            68          -0.01203        0.988\n",
      "            69          -0.01203        0.988\n",
      "            70          -0.01203        0.988\n",
      "            71          -0.01203        0.988\n",
      "            72          -0.01203        0.988\n",
      "            73          -0.01203        0.988\n",
      "            74          -0.01203        0.988\n",
      "            75          -0.01203        0.988\n",
      "            76          -0.01203        0.988\n",
      "            77          -0.01203        0.988\n",
      "            78          -0.01203        0.988\n",
      "            79          -0.01203        0.988\n",
      "            80          -0.01203        0.988\n",
      "            81          -0.01203        0.988\n",
      "            82          -0.01203        0.988\n",
      "            83          -0.01203        0.988\n",
      "            84          -0.01203        0.988\n",
      "            85          -0.01203        0.988\n",
      "            86          -0.01203        0.988\n",
      "            87          -0.01203        0.988\n",
      "            88          -0.01203        0.988\n",
      "            89          -0.01203        0.988\n",
      "            90          -0.01203        0.988\n",
      "            91          -0.01203        0.988\n",
      "            92          -0.01203        0.988\n",
      "            93          -0.01203        0.988\n",
      "            94          -0.01203        0.988\n",
      "            95          -0.01203        0.988\n",
      "            96          -0.01203        0.988\n",
      "            97          -0.01203        0.988\n",
      "            98          -0.01203        0.988\n",
      "            99          -0.01203        0.988\n",
      "         Final          -0.01203        0.988\n",
      "Classifier evaluation: \n",
      "Precision: None\n",
      "Recall: 0.0\n",
      "F-measure: None\n"
     ]
    }
   ],
   "source": [
    "# MaxentClassifier\n",
    "# Hyper paramters\n",
    "CUT=True\n",
    "SAMPLE_SIZE=500\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=True\n",
    "\n",
    "run_classifier(MaxentClassifier, categories[2][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure\n",
    "\n",
    "Running the Maxent classifier didn't work. It seems it is not \"learning\" anything. So I'll try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 3 - choosing between two classifiers (Bayes vs Decisition Tree)\n",
    "> chose category: gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.naivebayes.NaiveBayesClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 0.43243243243243246\n",
      "Recall: 0.8421052631578947\n",
      "F-measure: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "# Hyper paramters\n",
    "CUT=True\n",
    "SAMPLE_SIZE=2500\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(NaiveBayesClassifier, categories[2][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.decisiontree.DecisionTreeClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 1.0\n",
      "Recall: 0.5789473684210527\n",
      "F-measure: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "# Hyper paramters\n",
    "CUT=True\n",
    "SAMPLE_SIZE=1250\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(DecisionTreeClassifier, categories[2][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Opposing the classifiers\n",
    "\n",
    "For this test, we make the hypothesis that the best hyper parameters (3a) from the two previous classes are the best for each respective classifier.\n",
    "\n",
    "Let's confront them.\n",
    "\n",
    "#### Variant A: Naive Bayes classifier\n",
    "\n",
    "Without Lemmatization:\n",
    "\n",
    "* Precision: 0.43243243243243246\n",
    "* Recall: 0.8421052631578947\n",
    "* F-measure: 0.5714285714285714\n",
    "\n",
    "With  Lemmatization:\n",
    "\n",
    "* Precision: 0.5161290322580645\n",
    "* Recall: 0.8421052631578947\n",
    "* F-measure: 0.64\n",
    "\n",
    "#### Variant B: Decision Tree Classifier\n",
    "\n",
    "Without Lemmatization:\n",
    "\n",
    "* Precision: 1.0\n",
    "* Recall: 0.5789473684210527\n",
    "* F-measure: 0.7333333333333333\n",
    "\n",
    "With Lemmatization:\n",
    "\n",
    "* Precision: 1.0\n",
    "* Recall: 0.5263157894736842\n",
    "* F-measure: 0.689655172413793\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "It seems that decision tree as a better precision, but a worse recall than Bayes.\n",
    "\n",
    "As for the important bit: the f-measure is slightly better with decision tree.\n",
    "\n",
    "I tried to add/remove lemmatization since it was previously not a good addition. And it still isn't for Decision Tree, but it does help Bayes.\n",
    "\n",
    "The best variant is: Decision Tree and no lemmatization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.decisiontree.DecisionTreeClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: None\n",
      "Recall: 0.0\n",
      "F-measure: None\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "# Hyper paramters\n",
    "CUT=True\n",
    "SAMPLE_SIZE=1250\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(DecisionTreeClassifier, categories[2][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Vector choices\n",
    "\n",
    "Let's see what happens when we change the picking method for the word vectors.\n",
    "\n",
    "#### Variant A: Previous best\n",
    "\n",
    "* Precision: 1.0\n",
    "* Recall: 0.5789473684210527\n",
    "* F-measure: 0.7333333333333333\n",
    "\n",
    "#### Variant B: Picking the 2nd most common set of words\n",
    "\n",
    "* Precision: 1.0\n",
    "* Recall: 0.5263157894736842\n",
    "* F-measure: 0.689655172413793\n",
    "\n",
    "#### Variant C: Picking the 3rd most common set of words\n",
    "\n",
    "* Precision: 0.6666666666666666\n",
    "* Recall: 0.3157894736842105\n",
    "* F-measure: 0.42857142857142855\n",
    "\n",
    "#### Variant D: Picking the least common words\n",
    " \n",
    "* Precision: None\n",
    "* Recall: 0.0\n",
    "* F-measure: None\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "In this experience, we try to see if selecting least common words helps the decision tree classifier.\n",
    "\n",
    "By reasoning, one could say that less frequent words would be more discriminant. However, if they never appear in the texts (precisly we talk about a very low frequency), they might prevent the classifier to learn.\n",
    "\n",
    "As observed here, most common words are best suited. Least commons words fails totally.\n",
    "\n",
    "A better way for selecting words would be to use tf/idf or similar approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function adaptation\n",
    "\n",
    "some functions had to be changed in order to adapt it to having multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def _category_b(fileid, wanted_categories):\n",
    "    \"\"\"\n",
    "    return a specific category given a fileid. otherwise 'other' or 'ambiguous' (if multiple)\n",
    "    \"\"\"\n",
    "        \n",
    "    # found = reduce(lambda acc, c: acc.append(c) if c in reuters.categories(fileid) else acc, wanted_categories, [])\n",
    "    found = []\n",
    "    for c in wanted_categories:\n",
    "        if c in reuters.categories(fileid):\n",
    "            found.append(c)\n",
    "    \n",
    "    if len(found) == 1:\n",
    "        return found[0]\n",
    "    elif len(found) == 0:\n",
    "        return 'other'\n",
    "    else:\n",
    "        return 'ambiguous'\n",
    "\n",
    "\n",
    "def prepare_reuters_data_b(vocab_dic, wanted_categories, rm_punctuation, rm_stopwords, lemmatize):\n",
    "    __train_data = []\n",
    "    __test_data = []\n",
    "\n",
    "    for fileid in reuters.fileids():\n",
    "        category = _category_b(fileid, wanted_categories)\n",
    "\n",
    "        words = reuters.words(fileid)\n",
    "        words = filter_words(words, rm_punctuation, rm_stopwords, lemmatize)\n",
    "        dic = _words_dic(vocab_dic, words)\n",
    "        if fileid.startswith('test'):\n",
    "            __train_data.append([dic, category])\n",
    "        else:\n",
    "            __test_data.append([dic, category])\n",
    "                \n",
    "    return __test_data, __test_data\n",
    "\n",
    "\n",
    "def obtain_data_b(wanted_categories, cut, sampleSize, rm_punctuation, rm_stopwords, lemmatize):\n",
    "    _vocab_dic = vocab_dic(get_reuters_vocab(sampleSize, rm_punctuation, rm_stopwords,  lemmatize))\n",
    "    \n",
    "    _train_data, _test_data = prepare_reuters_data_b(_vocab_dic, wanted_categories, rm_punctuation, rm_stopwords,  lemmatize)\n",
    "\n",
    "    if cut:\n",
    "        _train_data, _test_data = prepare_pre_cut(_train_data, _test_data)\n",
    "        \n",
    "    return _train_data, _test_data\n",
    "\n",
    "\n",
    "def eval_classifier_b(trained_classifier, test_data):\n",
    "    \"\"\"\n",
    "    Inspiration: https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/\n",
    "    \"\"\"\n",
    "    ref_set = collections.defaultdict(set)\n",
    "    test_set = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, true_label) in enumerate(test_data):\n",
    "        if(true_label == 'ambiguous'):\n",
    "            print(true_label)\n",
    "        ref_set[true_label].add(i)\n",
    "        observed = trained_classifier.classify(feats)\n",
    "        test_set[observed].add(i)\n",
    "\n",
    "    print('Classifier evaluation: ')\n",
    "    \n",
    "    for label in ref_set.keys():\n",
    "        print(label)\n",
    "        print( 'Precision:', precision(ref_set[label], test_set[label]) )\n",
    "        print( 'Recall:', recall(ref_set[label], test_set[label]) )\n",
    "        print( 'F-measure:', f_measure(ref_set[label], test_set[label]) )  \n",
    "        print('---------------')\n",
    "    \n",
    "\n",
    "def run_classifier_b(classifier, wanted_categories, cut=True, sampleSize=500, rm_punctuation=True, rm_stopwords=True, lemmatize=True):\n",
    "    train_data, test_data = obtain_data_b(wanted_categories, cut, sampleSize, rm_punctuation, rm_stopwords,  lemmatize)\n",
    "    \n",
    "    print(f'{classifier}')\n",
    "    \n",
    "    trained_classifier = None\n",
    "    if f'{classifier}' == \"<class 'nltk.classify.maxent.MaxentClassifier'>\":\n",
    "        print('Hello Maxent')\n",
    "        trained_classifier = classifier.train(train_data, trace=3)\n",
    "    else:\n",
    "        trained_classifier = classifier.train(train_data)\n",
    "    \n",
    "    eval_classifier_b(trained_classifier, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.decisiontree.DecisionTreeClassifier'>\n",
      "ambiguous\n",
      "ambiguous\n",
      "Classifier evaluation: \n",
      "other\n",
      "Precision: 0.9665970772442589\n",
      "Recall: 0.9981316470250072\n",
      "F-measure: 0.9821112918051333\n",
      "---------------\n",
      "money-fx\n",
      "Precision: 0.9686609686609686\n",
      "Recall: 0.6343283582089553\n",
      "F-measure: 0.7666290868094701\n",
      "---------------\n",
      "corn\n",
      "Precision: 0.9888888888888889\n",
      "Recall: 0.9888888888888889\n",
      "F-measure: 0.9888888888888888\n",
      "---------------\n",
      "gold\n",
      "Precision: 1.0\n",
      "Recall: 0.5483870967741935\n",
      "F-measure: 0.7083333333333333\n",
      "---------------\n",
      "ambiguous\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F-measure: 1.0\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "# Hyper paramters\n",
    "CUT=False\n",
    "SAMPLE_SIZE=1250\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "WANTED_CATEGORIES = ['money-fx', 'corn', 'gold']\n",
    "\n",
    "run_classifier_b(DecisionTreeClassifier, WANTED_CATEGORIES, CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.naivebayes.NaiveBayesClassifier'>\n",
      "ambiguous\n",
      "ambiguous\n",
      "Classifier evaluation: \n",
      "other\n",
      "Precision: 0.9791666666666666\n",
      "Recall: 0.8646162690428284\n",
      "F-measure: 0.9183330789192489\n",
      "---------------\n",
      "money-fx\n",
      "Precision: 0.4994232987312572\n",
      "Recall: 0.8078358208955224\n",
      "F-measure: 0.6172487526728438\n",
      "---------------\n",
      "corn\n",
      "Precision: 0.2552504038772213\n",
      "Recall: 0.8777777777777778\n",
      "F-measure: 0.3954943679599499\n",
      "---------------\n",
      "gold\n",
      "Precision: 0.6923076923076923\n",
      "Recall: 0.967741935483871\n",
      "F-measure: 0.8071748878923767\n",
      "---------------\n",
      "ambiguous\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-measure: 0\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "# Hyper paramters\n",
    "CUT=False\n",
    "SAMPLE_SIZE=2500\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "WANTED_CATEGORIES = ['money-fx', 'corn', 'gold']\n",
    "\n",
    "run_classifier_b(NaiveBayesClassifier, WANTED_CATEGORIES, CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.naivebayes.NaiveBayesClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 0.46730975348338694\n",
      "Recall: 0.8104089219330854\n",
      "F-measure: 0.592794017675051\n",
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.decisiontree.DecisionTreeClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 0.988950276243094\n",
      "Recall: 0.988950276243094\n",
      "F-measure: 0.9889502762430941\n",
      "Number of words in reuters: 1720901 total - unique: 41600 \n",
      "\n",
      "After removals: 964625 total - unique: 30778 \n",
      "\n",
      "<class 'nltk.classify.decisiontree.DecisionTreeClassifier'>\n",
      "Classifier evaluation: \n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "F-measure: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "# Naive Bayes classifier\n",
    "# Hyper paramters\n",
    "CUT=False\n",
    "SAMPLE_SIZE=2500\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(NaiveBayesClassifier, categories[0][0], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)\n",
    "\n",
    "# 2\n",
    "\n",
    "# Decision Tree Classifier\n",
    "# Hyper paramters\n",
    "CUT=False\n",
    "SAMPLE_SIZE=1250\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(DecisionTreeClassifier, categories[1][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)\n",
    "\n",
    "# 3\n",
    "\n",
    "# Decision Tree Classifier\n",
    "# Hyper paramters\n",
    "CUT=False\n",
    "SAMPLE_SIZE=1250\n",
    "RM_PONCTUATION=True\n",
    "RM_STOPWORDS=True\n",
    "LEMMATIZE=False\n",
    "\n",
    "run_classifier(DecisionTreeClassifier, categories[2][2], CUT, SAMPLE_SIZE, RM_PONCTUATION, RM_STOPWORDS, LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4b\n",
    "\n",
    "Comparing between Bayes and Decision Tree again. Hypothese that previous hyper parameters are the best.\n",
    "\n",
    "This time, we have to classify with multiple labels/categories. To this end, we annotate the later or 'other' if none was found.\n",
    "\n",
    "In case of an overlap, I chose to add and 'ambiguous' tag. I chose this method to count it differently. Doing so allows to avoid a bias from the system correctly picking a category but getting rejected because we chose arbitrary the other (first found or random selection). We could improve by making overlap categories for each, but then we would have to find a way to count an half success if only one category on the two was found. For this experiment we'll not go that far.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Here is a recap of the final results done on all the data available (80% training and 20% test).\n",
    "\n",
    "\n",
    "|                                 | Precision | Recall | F-Measure |\n",
    "| ------------------------------- | --------- | ------ | --------- |\n",
    "| (4a 1) Naive Bayes: money-fx    | 0.467     | 0.810  | 0.592     |\n",
    "| (4a 2) Decision Tree: corn      | 0.988     | 0.988  | 0.988     |\n",
    "| (4a 3) Decision Tree: gold      | 1.0       | 0.5    | 0.666     |\n",
    "|                                 |           |        |           |\n",
    "| (4b 3) Decision Tree: money-fx  | 0.968     | 0.634  | 0.766     |\n",
    "| (4b 3) Decision Tree: corn      | 0.988     | 0.988  | 0.988     |\n",
    "| (4b 3) Decision Tree: gold      | 1.0       | 0.548  | 0.708     |\n",
    "| (4b 3) Decision Tree: ambiguous | 1.0       | 1.0    | 1.0       |\n",
    "\n",
    "\n",
    "As we can see, our multi-class system is able to identify exactly in the same maner the 2nd class (4a 2).\n",
    "\n",
    "It does perform better on money-fx because we used Naive Bayes (who gives less precise results in our empirical experiments).\n",
    "\n",
    "Surprisingly, the 4b system performs better than 4a3 on the class 'gold': the recall is a bit higher. We could make the theory that having the other categories helps it to build a bit of \"confidence\" of some sort.\n",
    "\n",
    "There is very few cases of ambiguous-labeled dispatches (3-4 maximum?). So we can't assume that having 1.0 F-Score is a good sign. It could just be some \"good luck\". \n",
    "\n",
    "From this final evaluation, we can conclude that the 4b system is better, but could certainly be tuned and improved a bit more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
